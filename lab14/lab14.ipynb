{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:31:24.491695Z",
     "end_time": "2026-01-21T15:31:24.554454Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# 1. Configuration and Data Loading\n",
    "BATCH_SIZE = 4\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:33:27.296988Z",
     "end_time": "2026-01-21T15:33:27.323975Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"student_learning_trajectory.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:33:27.656983Z",
     "end_time": "2026-01-21T15:33:27.718980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "   student_id  week  study_hours  sleep_hours  stress_level  attendance_rate  \\\n0           0     1     7.888120     7.318902      5.558083         0.951052   \n1           0     2     9.950370     5.987169      5.628495         0.759198   \n2           0     3     7.849006     7.375698      3.798723         0.820831   \n3           0     4     8.994472     7.822545      2.558313         0.870886   \n4           0     5     7.990915     5.416706      6.547401         0.796186   \n\n   screen_time_hours  caffeine_intake  learning_efficiency  fatigue_index  \\\n0           3.838244                1             0.425517       0.555808   \n1           2.175393                2             0.440707       0.900460   \n2           3.796587                0             0.453817       0.379872   \n3           1.080660                1             0.468138       0.255831   \n4           2.306644                1             0.481408       1.182505   \n\n   quiz_score  assignment_score  performance_index  \n0   63.270200        109.638265          91.091039  \n1   79.333452        113.540690          99.857795  \n2   77.072262        112.927018          98.585116  \n3   74.500142        119.813462         101.688134  \n4   57.751540        117.716967          93.730796  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>week</th>\n      <th>study_hours</th>\n      <th>sleep_hours</th>\n      <th>stress_level</th>\n      <th>attendance_rate</th>\n      <th>screen_time_hours</th>\n      <th>caffeine_intake</th>\n      <th>learning_efficiency</th>\n      <th>fatigue_index</th>\n      <th>quiz_score</th>\n      <th>assignment_score</th>\n      <th>performance_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>7.888120</td>\n      <td>7.318902</td>\n      <td>5.558083</td>\n      <td>0.951052</td>\n      <td>3.838244</td>\n      <td>1</td>\n      <td>0.425517</td>\n      <td>0.555808</td>\n      <td>63.270200</td>\n      <td>109.638265</td>\n      <td>91.091039</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2</td>\n      <td>9.950370</td>\n      <td>5.987169</td>\n      <td>5.628495</td>\n      <td>0.759198</td>\n      <td>2.175393</td>\n      <td>2</td>\n      <td>0.440707</td>\n      <td>0.900460</td>\n      <td>79.333452</td>\n      <td>113.540690</td>\n      <td>99.857795</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>3</td>\n      <td>7.849006</td>\n      <td>7.375698</td>\n      <td>3.798723</td>\n      <td>0.820831</td>\n      <td>3.796587</td>\n      <td>0</td>\n      <td>0.453817</td>\n      <td>0.379872</td>\n      <td>77.072262</td>\n      <td>112.927018</td>\n      <td>98.585116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>4</td>\n      <td>8.994472</td>\n      <td>7.822545</td>\n      <td>2.558313</td>\n      <td>0.870886</td>\n      <td>1.080660</td>\n      <td>1</td>\n      <td>0.468138</td>\n      <td>0.255831</td>\n      <td>74.500142</td>\n      <td>119.813462</td>\n      <td>101.688134</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>5</td>\n      <td>7.990915</td>\n      <td>5.416706</td>\n      <td>6.547401</td>\n      <td>0.796186</td>\n      <td>2.306644</td>\n      <td>1</td>\n      <td>0.481408</td>\n      <td>1.182505</td>\n      <td>57.751540</td>\n      <td>117.716967</td>\n      <td>93.730796</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:33:28.096986Z",
     "end_time": "2026-01-21T15:33:28.117980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['student_id', 'week', 'study_hours', 'sleep_hours', 'stress_level',\n       'attendance_rate', 'screen_time_hours', 'caffeine_intake',\n       'learning_efficiency', 'fatigue_index', 'quiz_score',\n       'assignment_score', 'performance_index'],\n      dtype='object')"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:33:28.901914Z",
     "end_time": "2026-01-21T15:33:28.939904Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "pivot_result = pd.pivot_table(\n",
    "    df,\n",
    "    values=[\"study_hours\", \"sleep_hours\", \"stress_level\", \"attendance_rate\", \"screen_time_hours\",\n",
    "            \"caffeine_intake\", \"learning_efficiency\", \"fatigue_index\", \"quiz_score\", \"assignment_score\",\n",
    "            \"performance_index\"],\n",
    "    index=[\"student_id\", \"week\"],\n",
    "    aggfunc=\"mean\",\n",
    "    fill_value=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:33:29.611126Z",
     "end_time": "2026-01-21T15:33:29.653128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "x_col_names = [\"study_hours\", \"sleep_hours\", \"stress_level\", \"attendance_rate\", \"screen_time_hours\",\n",
    "               \"caffeine_intake\", \"learning_efficiency\", \"fatigue_index\", \"quiz_score\", \"assignment_score\"]\n",
    "y_col_names = \"performance_index\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:33:30.427806Z",
     "end_time": "2026-01-21T15:33:30.474790Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "      study_hours  sleep_hours  stress_level  attendance_rate  \\\nweek                                                            \n1        7.888120     7.318902      5.558083         0.951052   \n2        9.950370     5.987169      5.628495         0.759198   \n3        7.849006     7.375698      3.798723         0.820831   \n4        8.994472     7.822545      2.558313         0.870886   \n5        7.990915     5.416706      6.547401         0.796186   \n6        9.887748     6.160782      4.381575         0.883126   \n7        9.834795     7.296561      2.924340         0.842420   \n8        8.939136     7.087047      4.401985         0.859176   \n9        8.191506     6.498243      6.830804         0.882875   \n10       8.632174     7.296120      5.522111         0.850511   \n11       9.214939     4.977685      3.113886         0.990396   \n12       8.567391     6.807639      5.603095         0.846529   \n13      10.000000     8.392326      3.238718         0.857689   \n14       9.873589     5.449337      5.137126         0.743770   \n15       8.643255     7.556122      7.087721         0.902645   \n16      10.000000     7.521942      5.593969         0.875049   \n\n      screen_time_hours  caffeine_intake  learning_efficiency  fatigue_index  \\\nweek                                                                           \n1              3.838244              1.0             0.425517       0.555808   \n2              2.175393              2.0             0.440707       0.900460   \n3              3.796587              0.0             0.453817       0.379872   \n4              1.080660              1.0             0.468138       0.255831   \n5              2.306644              1.0             0.481408       1.182505   \n6              6.951090              1.0             0.496545       0.717897   \n7              6.945927              3.0             0.511636       0.292434   \n8              1.024862              2.0             0.525903       0.440199   \n9              3.940480              4.0             0.539395       0.850333   \n10             4.530826              1.0             0.553353       0.552211   \n11             4.962898              1.0             0.567885       0.985494   \n12             2.662644              3.0             0.581777       0.624430   \n13             4.013135              1.0             0.597009       0.323872   \n14             5.947185              3.0             0.612133       1.030600   \n15             7.727773              1.0             0.626103       0.708772   \n16             5.692896              1.0             0.641335       0.559397   \n\n      quiz_score  assignment_score  \nweek                                \n1      63.270200        109.638265  \n2      79.333452        113.540690  \n3      77.072262        112.927018  \n4      74.500142        119.813462  \n5      57.751540        117.716967  \n6      63.487673        114.420171  \n7      76.623424        116.342893  \n8      66.458878        123.416689  \n9      68.838832        120.174227  \n10     67.884062        115.853122  \n11     58.928640        129.132620  \n12     73.446340        115.789208  \n13     78.965009        129.919608  \n14     64.891978        113.319200  \n15     82.512575        117.611052  \n16     74.816880        119.964567  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_hours</th>\n      <th>sleep_hours</th>\n      <th>stress_level</th>\n      <th>attendance_rate</th>\n      <th>screen_time_hours</th>\n      <th>caffeine_intake</th>\n      <th>learning_efficiency</th>\n      <th>fatigue_index</th>\n      <th>quiz_score</th>\n      <th>assignment_score</th>\n    </tr>\n    <tr>\n      <th>week</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>7.888120</td>\n      <td>7.318902</td>\n      <td>5.558083</td>\n      <td>0.951052</td>\n      <td>3.838244</td>\n      <td>1.0</td>\n      <td>0.425517</td>\n      <td>0.555808</td>\n      <td>63.270200</td>\n      <td>109.638265</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9.950370</td>\n      <td>5.987169</td>\n      <td>5.628495</td>\n      <td>0.759198</td>\n      <td>2.175393</td>\n      <td>2.0</td>\n      <td>0.440707</td>\n      <td>0.900460</td>\n      <td>79.333452</td>\n      <td>113.540690</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.849006</td>\n      <td>7.375698</td>\n      <td>3.798723</td>\n      <td>0.820831</td>\n      <td>3.796587</td>\n      <td>0.0</td>\n      <td>0.453817</td>\n      <td>0.379872</td>\n      <td>77.072262</td>\n      <td>112.927018</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8.994472</td>\n      <td>7.822545</td>\n      <td>2.558313</td>\n      <td>0.870886</td>\n      <td>1.080660</td>\n      <td>1.0</td>\n      <td>0.468138</td>\n      <td>0.255831</td>\n      <td>74.500142</td>\n      <td>119.813462</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7.990915</td>\n      <td>5.416706</td>\n      <td>6.547401</td>\n      <td>0.796186</td>\n      <td>2.306644</td>\n      <td>1.0</td>\n      <td>0.481408</td>\n      <td>1.182505</td>\n      <td>57.751540</td>\n      <td>117.716967</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>9.887748</td>\n      <td>6.160782</td>\n      <td>4.381575</td>\n      <td>0.883126</td>\n      <td>6.951090</td>\n      <td>1.0</td>\n      <td>0.496545</td>\n      <td>0.717897</td>\n      <td>63.487673</td>\n      <td>114.420171</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9.834795</td>\n      <td>7.296561</td>\n      <td>2.924340</td>\n      <td>0.842420</td>\n      <td>6.945927</td>\n      <td>3.0</td>\n      <td>0.511636</td>\n      <td>0.292434</td>\n      <td>76.623424</td>\n      <td>116.342893</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8.939136</td>\n      <td>7.087047</td>\n      <td>4.401985</td>\n      <td>0.859176</td>\n      <td>1.024862</td>\n      <td>2.0</td>\n      <td>0.525903</td>\n      <td>0.440199</td>\n      <td>66.458878</td>\n      <td>123.416689</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8.191506</td>\n      <td>6.498243</td>\n      <td>6.830804</td>\n      <td>0.882875</td>\n      <td>3.940480</td>\n      <td>4.0</td>\n      <td>0.539395</td>\n      <td>0.850333</td>\n      <td>68.838832</td>\n      <td>120.174227</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>8.632174</td>\n      <td>7.296120</td>\n      <td>5.522111</td>\n      <td>0.850511</td>\n      <td>4.530826</td>\n      <td>1.0</td>\n      <td>0.553353</td>\n      <td>0.552211</td>\n      <td>67.884062</td>\n      <td>115.853122</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>9.214939</td>\n      <td>4.977685</td>\n      <td>3.113886</td>\n      <td>0.990396</td>\n      <td>4.962898</td>\n      <td>1.0</td>\n      <td>0.567885</td>\n      <td>0.985494</td>\n      <td>58.928640</td>\n      <td>129.132620</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>8.567391</td>\n      <td>6.807639</td>\n      <td>5.603095</td>\n      <td>0.846529</td>\n      <td>2.662644</td>\n      <td>3.0</td>\n      <td>0.581777</td>\n      <td>0.624430</td>\n      <td>73.446340</td>\n      <td>115.789208</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>10.000000</td>\n      <td>8.392326</td>\n      <td>3.238718</td>\n      <td>0.857689</td>\n      <td>4.013135</td>\n      <td>1.0</td>\n      <td>0.597009</td>\n      <td>0.323872</td>\n      <td>78.965009</td>\n      <td>129.919608</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>9.873589</td>\n      <td>5.449337</td>\n      <td>5.137126</td>\n      <td>0.743770</td>\n      <td>5.947185</td>\n      <td>3.0</td>\n      <td>0.612133</td>\n      <td>1.030600</td>\n      <td>64.891978</td>\n      <td>113.319200</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>8.643255</td>\n      <td>7.556122</td>\n      <td>7.087721</td>\n      <td>0.902645</td>\n      <td>7.727773</td>\n      <td>1.0</td>\n      <td>0.626103</td>\n      <td>0.708772</td>\n      <td>82.512575</td>\n      <td>117.611052</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>10.000000</td>\n      <td>7.521942</td>\n      <td>5.593969</td>\n      <td>0.875049</td>\n      <td>5.692896</td>\n      <td>1.0</td>\n      <td>0.641335</td>\n      <td>0.559397</td>\n      <td>74.816880</td>\n      <td>119.964567</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_result.loc[0, x_col_names]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:34:24.358229Z",
     "end_time": "2026-01-21T15:34:24.427280Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "data_input = [\n",
    "    (pivot_result.loc[i, x_col_names].to_numpy(dtype=np.float32),\n",
    "    pivot_result.loc[i, y_col_names].to_numpy(dtype=np.float32))\n",
    "    for i in pivot_result.index.levels[0]\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:36:05.403939Z",
     "end_time": "2026-01-21T15:36:06.905861Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(data_input, [0.7, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:36:07.408214Z",
     "end_time": "2026-01-21T15:36:07.449095Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:36:08.040578Z",
     "end_time": "2026-01-21T15:36:08.137838Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:36:08.490803Z",
     "end_time": "2026-01-21T15:36:08.507254Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# 2. Define the Neural Network Model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Input layer (28*28 = 784 pixels) to a hidden layer of 32 neurons\n",
    "        self.fc1 = nn.Linear(16 * 10, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image from 28x28 to a 784-element vector\n",
    "        x = x.view(-1, 16 * 10)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # Apply log softmax for the final output (often used with NLLLoss, which is part of CrossEntropyLoss)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:37:57.981439Z",
     "end_time": "2026-01-21T15:37:58.013002Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "model = SimpleNN().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:37:58.680548Z",
     "end_time": "2026-01-21T15:37:58.729969Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# 3. Define Loss and Optimizer\n",
    "criterion = nn.NLLLoss()  # Negative Log-Likelihood Loss (suitable for log_softmax output)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T15:37:59.232295Z",
     "end_time": "2026-01-21T15:37:59.240428Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16])\n",
      "torch.Size([4, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[109], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(output\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(target\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 14\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calculate loss\u001B[39;00m\n\u001B[0;32m     15\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m# Backward pass (calculate gradients)\u001B[39;00m\n\u001B[0;32m     16\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()  \u001B[38;5;66;03m# Update weights\u001B[39;00m\n",
      "File \u001B[1;32mD:\\programming\\python\\2025-NN-AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1775\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\programming\\python\\2025-NN-AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1784\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1788\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\programming\\python\\2025-NN-AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:259\u001B[0m, in \u001B[0;36mNLLLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m    Runs the forward pass.\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    265\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\programming\\python\\2025-NN-AI\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:3143\u001B[0m, in \u001B[0;36mnll_loss\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[0;32m   3141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3142\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss_nd\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3144\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\n\u001B[0;32m   3145\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# 4. Training Loop\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        output = model(data)  # Forward pass\n",
    "        loss = criterion(output, target)  # Calculate loss\n",
    "        loss.backward()  # Backward pass (calculate gradients)\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch + 1}/{NUM_EPOCHS} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Testing the Model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "      f'({accuracy:.2f}%)\\n')\n",
    "\n",
    "# 6. Save the model (optional)\n",
    "torch.save(model.state_dict(), \"students_simple_nn.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
