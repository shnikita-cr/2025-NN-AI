{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:09:18.380564Z",
     "end_time": "2026-02-23T16:10:52.708124Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Configuration and Data Loading\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:10:52.708124Z",
     "end_time": "2026-02-23T16:10:52.817126Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SingleImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, extensions=('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.extensions = extensions\n",
    "\n",
    "        # Collect all image files with given extensions\n",
    "        self.paths = []\n",
    "        for ext in extensions:\n",
    "            self.paths.extend(self.root.glob(f'*{ext}'))\n",
    "        self.paths = sorted(self.paths)  # ensure consistent ordering\n",
    "\n",
    "        # Extract class names from file stems\n",
    "        self.classes = sorted({p.stem for p in self.paths})\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')   # ensure RGB for consistency\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load image at {img_path}: {e}\")\n",
    "\n",
    "        # Get class label from filename stem\n",
    "        class_name = img_path.stem\n",
    "        label = self.class_to_idx[class_name]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:32:27.490308Z",
     "end_time": "2026-02-23T16:32:27.530334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(\n",
    "    root='./data/asl_alphabet_train',\n",
    "    transform=data_transform\n",
    ")\n",
    "test_data = SingleImageFolder(\n",
    "    root=\"./data/asl_alphabet_test\",\n",
    "    transform=data_transform\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:32:28.138297Z",
     "end_time": "2026-02-23T16:32:28.599296Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset=test_data,\n",
    "                             shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T17:56:39.615746Z",
     "end_time": "2026-02-23T17:56:39.652809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:32:30.844297Z",
     "end_time": "2026-02-23T16:32:30.852297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Define the Neural Network Model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # 200 200\n",
    "        self.pool0 = nn.MaxPool2d(kernel_size=4) # 50 50\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3)  #48 48\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)  # 24 24\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=3)  #22 22\n",
    "        self.pool2 = nn.AdaptiveAvgPool2d(output_size=(3, 3))\n",
    "        # self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        self.fc3 = nn.Linear(4 * 3 * 3, 29)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool0(x)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 4 * 3 * 3)\n",
    "        x = self.fc3(x)\n",
    "        # Apply log softmax for the final output (often used with NLLLoss, which is part of CrossEntropyLoss)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = SimpleNN().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:32:31.759298Z",
     "end_time": "2026-02-23T16:32:31.879305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Define Loss and Optimizer\n",
    "criterion = nn.NLLLoss()  # Negative Log-Likelihood Loss (suitable for log_softmax output)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:32:32.846299Z",
     "end_time": "2026-02-23T16:32:32.859295Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:32:33.617298Z",
     "end_time": "2026-02-23T16:32:33.707294Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "writer = SummaryWriter(f'runs/init_{datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:40:10.603189Z",
     "end_time": "2026-02-23T16:40:10.635154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    \"\"\"Convert seconds to HH:MM:SS format.\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:40:11.427155Z",
     "end_time": "2026-02-23T16:40:11.440161Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Training Loop\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    epoch_start = time.time()\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            elapsed = time.time() - epoch_start\n",
    "            batches_done = batch_idx + 1\n",
    "            # Avoid division by zero\n",
    "            avg_time_per_batch = elapsed / batches_done\n",
    "            remaining_batches = total_batches - batches_done\n",
    "            remaining = avg_time_per_batch * remaining_batches\n",
    "\n",
    "            elapsed_str = format_time(elapsed)\n",
    "            remaining_str = format_time(max(remaining, 0))\n",
    "\n",
    "            print(f'Train Epoch: {epoch + 1}/{NUM_EPOCHS} '\n",
    "                  f'[{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / total_batches:.0f}%)]\\t'\n",
    "                  f'Loss: {loss.item():.6f}\\t'\n",
    "                  f'Elapsed: {elapsed_str}\\t'\n",
    "                  f'Remaining: {remaining_str}')\n",
    "\n",
    "    avg_train_loss = running_loss / total_batches\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f'Epoch {epoch+1} completed in {format_time(epoch_time)}')\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T16:40:12.268078Z",
     "end_time": "2026-02-23T17:44:06.914426Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T17:44:07.001973Z",
     "end_time": "2026-02-23T17:44:07.202680Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T17:44:07.202680Z",
     "end_time": "2026-02-23T17:44:07.438475Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tensorboard --logdir=runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T17:44:07.461734Z",
     "end_time": "2026-02-23T17:44:07.497842Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Testing the Model\n",
    "loss_items = {}\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss_items[test_data.classes[target]] = [test_loss]\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "      f'({accuracy:.2f}%)\\n')\n",
    "loss_df = pd.DataFrame(data=loss_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T18:09:30.946195Z",
     "end_time": "2026-02-23T18:09:31.044194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_df.T.sort_index().plot(kind=\"bar\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T18:09:32.245196Z",
     "end_time": "2026-02-23T18:09:32.573191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model (optional)\n",
    "torch.save(model.state_dict(), \"mnist_simple_nn.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-23T18:09:36.365659Z",
     "end_time": "2026-02-23T18:09:36.561968Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def export_onnx(model, onnx_path=\"dynamic_cnn.onnx\"):\n",
    "    dummy = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy,\n",
    "            onnx_path,\n",
    "            opset_version=18,\n",
    "            dynamo=False,\n",
    "            export_params=True,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[\"input\"],\n",
    "            output_names=[\"output\"],\n",
    "            dynamic_axes={\n",
    "                \"input\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
    "                \"output\": {0: \"batch\"},\n",
    "            },\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T20:23:02.089939Z",
     "end_time": "2026-02-11T20:23:02.110214Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onnx_path = \"model.onnx\"\n",
    "export_onnx(model, onnx_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T20:01:20.772970Z",
     "end_time": "2026-02-11T20:01:20.818965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)  # Check for model correctness\n",
    "\n",
    "print(\"Model inputs:\")\n",
    "for input in onnx_model.graph.input:\n",
    "    print(f\"Input name: {input.name}\")\n",
    "    print(\n",
    "        f\"Input shape: {[(d.dim_value if d.HasField('dim_value') else d.dim_param) for d in input.type.tensor_type.shape.dim]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T20:01:20.820958Z",
     "end_time": "2026-02-11T20:01:20.831961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take a look at model graph\n",
    "# https://netron.app/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T20:01:20.833962Z",
     "end_time": "2026-02-11T20:01:20.874960Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
