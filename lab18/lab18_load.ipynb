{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2026-02-11T18:37:01.959524Z",
     "end_time": "2026-02-11T18:37:05.216456Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Input layer (28*28 = 784 pixels) to a hidden layer of 128 neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Output layer (10 classes for digits 0-9)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image from 28x28 to a 784-element vector\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # Apply log softmax for the final output (often used with NLLLoss, which is part of CrossEntropyLoss)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SimpleNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = SimpleNN().to(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T18:37:15.126917Z",
     "end_time": "2026-02-11T18:37:15.173763Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T18:37:50.009924Z",
     "end_time": "2026-02-11T18:37:50.041352Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.load(\"mnist_simple_nn.pth\",\n",
    "           map_location=torch.device(\"cpu\"),\n",
    "           weights_only=True,\n",
    "           )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T18:42:49.206639Z",
     "end_time": "2026-02-11T18:42:49.262660Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"mnist_simple_nn.pth\",\n",
    "                                 map_location=torch.device(\"cpu\"),\n",
    "                                 weights_only=True,\n",
    "                                 ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T18:42:11.086914Z",
     "end_time": "2026-02-11T18:42:11.155583Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(size=(28,28))\n",
    "    y = model(x)\n",
    "    print(y)\n",
    "    print(y.max())\n",
    "    plt.imshow(x, cmap=\"gray\")\n",
    "    plt.title(y.argmax())\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T18:49:52.351076Z",
     "end_time": "2026-02-11T18:49:52.466095Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
