{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:52.767651Z",
     "end_time": "2026-02-11T15:37:52.875847Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Configuration and Data Loading\n",
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize with MNIST mean (0.1307) and std (0.3081)\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:52.775652Z",
     "end_time": "2026-02-11T15:37:52.954848Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download and load training data\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Download and load test data\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:52.790848Z",
     "end_time": "2026-02-11T15:37:52.977127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:52.854848Z",
     "end_time": "2026-02-11T15:37:52.982128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Define the Neural Network Model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Input layer (28*28 = 784 pixels) to a hidden layer of 128 neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Output layer (10 classes for digits 0-9)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image from 28x28 to a 784-element vector\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # Apply log softmax for the final output (often used with NLLLoss, which is part of CrossEntropyLoss)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = SimpleNN().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:52.873846Z",
     "end_time": "2026-02-11T15:37:52.982128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Define Loss and Optimizer\n",
    "criterion = nn.NLLLoss()  # Negative Log-Likelihood Loss (suitable for log_softmax output)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:52.902847Z",
     "end_time": "2026-02-11T15:37:52.983129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:52.919848Z",
     "end_time": "2026-02-11T15:37:52.983129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/init')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:52.933846Z",
     "end_time": "2026-02-11T15:37:52.983129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Training Loop\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        output = model(data)  # Forward pass\n",
    "        loss = criterion(output, target)  # Calculate loss\n",
    "        loss.backward()  # Backward pass (calculate gradients)\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch + 1}/{NUM_EPOCHS} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:36:10.961546Z",
     "end_time": "2026-02-11T15:37:37.331017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:37.332017Z",
     "end_time": "2026-02-11T15:37:37.347017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:37.349016Z",
     "end_time": "2026-02-11T15:37:37.423014Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tensorboard --logdir=runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:37.365018Z",
     "end_time": "2026-02-11T15:37:37.455017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Testing the Model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "      f'({accuracy:.2f}%)\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:37.381017Z",
     "end_time": "2026-02-11T15:37:39.596516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model (optional)\n",
    "torch.save(model.state_dict(), \"mnist_simple_nn.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:39.599515Z",
     "end_time": "2026-02-11T15:37:39.611513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:39.611513Z",
     "end_time": "2026-02-11T15:37:39.656545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def export_onnx(model, onnx_path=\"dynamic_cnn.onnx\"):\n",
    "    dummy = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy,\n",
    "            onnx_path,\n",
    "            opset_version=18,\n",
    "            dynamo=False,\n",
    "            export_params=True,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[\"input\"],\n",
    "            output_names=[\"output\"],\n",
    "            dynamic_axes={\n",
    "                \"input\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
    "                \"output\": {0: \"batch\"},\n",
    "            },\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:39.630514Z",
     "end_time": "2026-02-11T15:37:39.666516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onnx_path = \"model.onnx\"\n",
    "export_onnx(model, onnx_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:39.644516Z",
     "end_time": "2026-02-11T15:37:39.720535Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)  # Check for model correctness\n",
    "\n",
    "print(\"Model inputs:\")\n",
    "for input in onnx_model.graph.input:\n",
    "    print(f\"Input name: {input.name}\")\n",
    "    print(\n",
    "        f\"Input shape: {[(d.dim_value if d.HasField('dim_value') else d.dim_param) for d in input.type.tensor_type.shape.dim]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:39.690514Z",
     "end_time": "2026-02-11T15:37:39.731516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take a look at model graph\n",
    "# https://netron.app/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:39.706513Z",
     "end_time": "2026-02-11T15:37:39.731516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-02-11T15:37:39.723514Z",
     "end_time": "2026-02-11T15:37:39.742513Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
