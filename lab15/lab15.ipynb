{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:36.731373Z",
     "end_time": "2026-01-21T19:56:36.825312Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.28604,), (0.32025,))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:37.462230Z",
     "end_time": "2026-01-21T19:56:37.542585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",  # Directory to save the data in\n",
    "    train=True,  # Specifies training dataset\n",
    "    download=True,  # Downloads the dataset if not already present\n",
    "    transform=transform  # Applies the defined transform\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:38.035758Z",
     "end_time": "2026-01-21T19:56:38.126490Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "# 3. Download and load the test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,  # Specifies test dataset\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 4. Create DataLoaders for batching and shuffling\n",
    "BATCH_SIZE = 64"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:38.915138Z",
     "end_time": "2026-01-21T19:56:38.945798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True  # Shuffles the data every epoch for better training\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False  # Shuffling not necessary for testing\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:39.624693Z",
     "end_time": "2026-01-21T19:56:39.655662Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in training loader: 938\n",
      "First batch image shape: torch.Size([64, 1, 28, 28])\n",
      "First batch label shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# 5. Iterate through a DataLoader (optional, for verification)\n",
    "print(f\"Number of batches in training loader: {len(train_loader)}\")\n",
    "first_batch_images, first_batch_labels = next(iter(train_loader))\n",
    "print(f\"First batch image shape: {first_batch_images.shape}\")\n",
    "print(f\"First batch label shape: {first_batch_labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:40.070473Z",
     "end_time": "2026-01-21T19:56:40.117339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([6, 8, 1, 0, 8, 9, 5, 1, 7, 3, 8, 0, 1, 8, 3, 8, 2, 1, 1, 1, 2, 3, 7, 7,\n        6, 3, 1, 7, 7, 3, 0, 3, 7, 6, 4, 6, 2, 3, 3, 1, 1, 8, 7, 9, 2, 9, 4, 4,\n        9, 1, 6, 1, 5, 1, 4, 2, 3, 1, 6, 8, 7, 2, 3, 5])"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:40.757476Z",
     "end_time": "2026-01-21T19:56:40.804375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:41.580229Z",
     "end_time": "2026-01-21T19:56:41.614318Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [],
   "source": [
    "# 2. Define the Neural Network Modelclass SimpleNN(nn.Module):\n",
    "class FashionNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fc1 = nn.Linear(28 * 28, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:42.307535Z",
     "end_time": "2026-01-21T19:56:42.338782Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "model = FashionNN().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:43.016773Z",
     "end_time": "2026-01-21T19:56:43.046284Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "# 3. Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:43.737754Z",
     "end_time": "2026-01-21T19:56:43.776490Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/5 [0/60000 (0%)]\tLoss: 2.407512\n",
      "Train Epoch: 1/5 [6400/60000 (11%)]\tLoss: 0.903699\n",
      "Train Epoch: 1/5 [12800/60000 (21%)]\tLoss: 0.516983\n",
      "Train Epoch: 1/5 [19200/60000 (32%)]\tLoss: 0.731142\n",
      "Train Epoch: 1/5 [25600/60000 (43%)]\tLoss: 0.401668\n",
      "Train Epoch: 1/5 [32000/60000 (53%)]\tLoss: 0.404762\n",
      "Train Epoch: 1/5 [38400/60000 (64%)]\tLoss: 0.371117\n",
      "Train Epoch: 1/5 [44800/60000 (75%)]\tLoss: 0.459133\n",
      "Train Epoch: 1/5 [51200/60000 (85%)]\tLoss: 0.285300\n",
      "Train Epoch: 1/5 [57600/60000 (96%)]\tLoss: 0.267680\n",
      "Train Epoch: 2/5 [0/60000 (0%)]\tLoss: 0.576227\n",
      "Train Epoch: 2/5 [6400/60000 (11%)]\tLoss: 0.380893\n",
      "Train Epoch: 2/5 [12800/60000 (21%)]\tLoss: 0.215644\n",
      "Train Epoch: 2/5 [19200/60000 (32%)]\tLoss: 0.766666\n",
      "Train Epoch: 2/5 [25600/60000 (43%)]\tLoss: 0.418699\n",
      "Train Epoch: 2/5 [32000/60000 (53%)]\tLoss: 0.352307\n",
      "Train Epoch: 2/5 [38400/60000 (64%)]\tLoss: 0.320565\n",
      "Train Epoch: 2/5 [44800/60000 (75%)]\tLoss: 0.358250\n",
      "Train Epoch: 2/5 [51200/60000 (85%)]\tLoss: 0.470961\n",
      "Train Epoch: 2/5 [57600/60000 (96%)]\tLoss: 0.497653\n",
      "Train Epoch: 3/5 [0/60000 (0%)]\tLoss: 0.472941\n",
      "Train Epoch: 3/5 [6400/60000 (11%)]\tLoss: 0.215454\n",
      "Train Epoch: 3/5 [12800/60000 (21%)]\tLoss: 0.351342\n",
      "Train Epoch: 3/5 [19200/60000 (32%)]\tLoss: 0.575240\n",
      "Train Epoch: 3/5 [25600/60000 (43%)]\tLoss: 0.389017\n",
      "Train Epoch: 3/5 [32000/60000 (53%)]\tLoss: 0.262020\n",
      "Train Epoch: 3/5 [38400/60000 (64%)]\tLoss: 0.409104\n",
      "Train Epoch: 3/5 [44800/60000 (75%)]\tLoss: 0.470521\n",
      "Train Epoch: 3/5 [51200/60000 (85%)]\tLoss: 0.348818\n",
      "Train Epoch: 3/5 [57600/60000 (96%)]\tLoss: 0.531205\n",
      "Train Epoch: 4/5 [0/60000 (0%)]\tLoss: 0.347646\n",
      "Train Epoch: 4/5 [6400/60000 (11%)]\tLoss: 0.307966\n",
      "Train Epoch: 4/5 [12800/60000 (21%)]\tLoss: 0.376922\n",
      "Train Epoch: 4/5 [19200/60000 (32%)]\tLoss: 0.271045\n",
      "Train Epoch: 4/5 [25600/60000 (43%)]\tLoss: 0.577046\n",
      "Train Epoch: 4/5 [32000/60000 (53%)]\tLoss: 0.266155\n",
      "Train Epoch: 4/5 [38400/60000 (64%)]\tLoss: 0.293785\n",
      "Train Epoch: 4/5 [44800/60000 (75%)]\tLoss: 0.534438\n",
      "Train Epoch: 4/5 [51200/60000 (85%)]\tLoss: 0.328575\n",
      "Train Epoch: 4/5 [57600/60000 (96%)]\tLoss: 0.444801\n",
      "Train Epoch: 5/5 [0/60000 (0%)]\tLoss: 0.375989\n",
      "Train Epoch: 5/5 [6400/60000 (11%)]\tLoss: 0.522826\n",
      "Train Epoch: 5/5 [12800/60000 (21%)]\tLoss: 0.307430\n",
      "Train Epoch: 5/5 [19200/60000 (32%)]\tLoss: 0.401153\n",
      "Train Epoch: 5/5 [25600/60000 (43%)]\tLoss: 0.457710\n",
      "Train Epoch: 5/5 [32000/60000 (53%)]\tLoss: 0.371467\n",
      "Train Epoch: 5/5 [38400/60000 (64%)]\tLoss: 0.444115\n",
      "Train Epoch: 5/5 [44800/60000 (75%)]\tLoss: 0.335570\n",
      "Train Epoch: 5/5 [51200/60000 (85%)]\tLoss: 0.429027\n",
      "Train Epoch: 5/5 [57600/60000 (96%)]\tLoss: 0.722804\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/init')\n",
    "running_loss = 0\n",
    "# 4. Training Loop\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        output = model(data)  # Forward pass\n",
    "        loss = criterion(output, target)  # Calculate loss\n",
    "        loss.backward()  # Backward pass (calculate gradients)\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch + 1}/{NUM_EPOCHS} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:56:44.443923Z",
     "end_time": "2026-01-21T19:58:54.740419Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "FashionNN(\n  (fc1): Linear(in_features=784, out_features=10, bias=True)\n)"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Testing the Model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:51:02.494814Z",
     "end_time": "2026-01-21T19:51:02.512204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "data": {
      "text/plain": "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nFashionNN                                --\n├─Linear: 1-1                            7,850\n=================================================================\nTotal params: 7,850\nTrainable params: 7,850\nNon-trainable params: 0\n================================================================="
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:51:02.515837Z",
     "end_time": "2026-01-21T19:51:02.587130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:51:02.546531Z",
     "end_time": "2026-01-21T19:51:06.743590Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4635, Accuracy: 8376/10000 (83.76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "      f'({accuracy:.2f}%)\\n')\n",
    "\n",
    "# 6. Save the model (optional)\n",
    "torch.save(model.state_dict(), \"fashion_nn.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2026-01-21T19:51:06.735504Z",
     "end_time": "2026-01-21T19:51:06.761275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
